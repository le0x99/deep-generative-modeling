{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b722b3c-7a15-41a6-a510-796e59108d50",
   "metadata": {},
   "source": [
    "### 0. Code\n",
    "\n",
    "The GAN, VAE and FID were implemented from scratch. The only thing we didnt manage to implement without any external code was the normalizing flow prior. For this we borrowed a small code snippet from Jakub. Overall our implementations differ greatly from what you can find online, but the models are working and we understand why and how they work (except for the NFP ;)).\n",
    "\n",
    "- GAN code : ./myGANS.py\n",
    "- VES code : ./myVAES.py\n",
    "- FID code : ./fid_score.py\n",
    "- misc.    : ./training_tools.py\n",
    "\n",
    "All experiments are fully reproducible.\n",
    "\n",
    "\n",
    "### 1. Training Report\n",
    "\n",
    "The report is structured into four annotated notebooks to make things easy to reproduce.\n",
    "\n",
    "0. GANS.ipynb\n",
    "    - Annotated GAN training.\n",
    "    \n",
    "    \n",
    "    \n",
    "1. VAES_NP.ipynb\n",
    "    - Annotated VAE training with Normal Prior.\n",
    "    \n",
    "    \n",
    "\n",
    "2. VAES_NFP.ipynb\n",
    "    - Annotated VAE training with Normalizing Flow Prior.\n",
    "    \n",
    "    \n",
    "    \n",
    "3. Test_analysis.ipynb\n",
    "    - Annotated inference on the test set.\n",
    "    - Linear interpolations in the latent space.\n",
    "\n",
    "### 2. Experimental Setup\n",
    "\n",
    "- For all experiments, we load all objects (including the datasets) into VRAM, i.e. we dont use dataloaders.\n",
    "- For all experiments, we use a **latent_dim of 50**. We tried out smaller latent dims, even a tiny latent vector of 2 works surprisingly well. However parameter tuning was done on the basis of 50, so we stick to that when presenting results.\n",
    "- **We focus on small architectures that are very basic and easy to reproduce**. To make the models truly comparable, **all of them have approximately the same number of parameters**, (GAN, NP, NFP) -> MNIST : (999_985 vs. 990_804 vs. 993_207 ) and for SVHN : (2_832_673 vs. 2_949_332 vs. 2_608_066 ). Note that balancing the parameters between the model is not easy, especially increasing or decreasing the parameters of the gan can have a huge impact because its sensitivity to hyperparameters. We also dont use any convolution/deconvolutions, because we wanted to focus on the true vanilla versions of the models ( We did run some experiments using conv layers tho).\n",
    "- During training time, we validated mostly by visual inspection of generated samples. Additionally, after each epoch, we draw a random batch of size 100 from the validation set and generate 100 images from the model to ask ICV3 for the FID. Although comparing samples of size 100 is extremely small, its the maximum feasable, since inference on ICV3 is very expensive and results in significant bottleneck when training. We collect these holdout fid scores to compare the models in a more equal way. In the final runs we dont collect the validation fid scores because it makes training times so much longer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_project",
   "language": "python",
   "name": "asr_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
